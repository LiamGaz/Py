

import pandas as pd
import numpy as np
import time
from sklearn import datasets, linear_model
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegressionCV
from sklearn.datasets import load_iris
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

#bootstrap data
df = pd.read_csv("df.csv", sep=";")
np.random.seed(2021)
boot = np.random.choice(len(df), size=100000)
data = df.iloc[boot]
data = data.iloc[: , 1:]
data['X1'] = data['X1'].str.replace(',','.').astype(float)
data['X2'] = data['X2'].str.replace(',','.').astype(float)
data['X3'] = data['X3'].astype(float)
data['X4'] = data['X4'].str.replace(',','.').astype(float)
data['Y'] = data['Y'].str.replace(',','.').astype(float)
x = data.iloc[: , 0:4]
y = data.iloc[: , 4:]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2021)

#GLM model newton cg train
start = time.process_time()
x_train, y_train = load_iris(return_X_y=True)
regr = LogisticRegressionCV(cv=5, random_state=2021, solver="newton-cg").fit(x_train, y_train)
print(regr.score(x_train, y_train))
print(time.process_time() - start)

#test
start = time.process_time()
x_test, y_test = load_iris(return_X_y=True)
print(regr.score(x_test, y_test))
print(time.process_time() - start)

#GLM model lilblinear train
start = time.process_time()
regr = LogisticRegressionCV(cv=5, random_state=2021, solver="liblinear").fit(x_train, y_train)
print(regr.score(x_train, y_train))
print(time.process_time() - start)

start = time.process_time()
x_test, y_test = load_iris(return_X_y=True)
print(regr.score(x_test, y_test))
print(time.process_time() - start)

#XGBoost package
from xgboost import XGBClassifier
from sklearn.utils import resample
from sklearn.model_selection import KFold, cross_val_score

#XGBoost model
start = time.process_time()
model = XGBClassifier()
model.fit(x_train, y_train)
kfold = KFold(n_splits=5, random_state=2021)
results = cross_val_score(model, x_train, y_train, cv=kfold)
print("Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))

y_pred = model.predict(x_test)
preds = [round(value) for value in y_pred]
accuracy = accuracy_score(y_test, preds)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
print(time.process_time() - start)

start = time.process_time()
model = XGBClassifier()
model.fit(x_train, y_train)
kfold = KFold(n_splits=10, random_state=2021)
results = cross_val_score(model, x_train, y_train, cv=kfold)
print("Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))

y_pred = model.predict(x_test)
preds = [round(value) for value in y_pred]
accuracy = accuracy_score(y_test, preds)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
print(time.process_time() - start)

#DL network
import pandas as pd
import numpy as np
import time
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
import tensorflow as tf

#DL model
start = time.process_time()
df = pd.read_csv("df.csv", sep=";")
np.random.seed(2021)
boot = np.random.choice(len(df), size=10000)
data = df.iloc[boot]
data = data.iloc[: , 1:]
data['X1'] = data['X1'].str.replace(',','.').astype(float)
data['X2'] = data['X2'].str.replace(',','.').astype(float)
data['X3'] = data['X3'].astype(float)
data['X4'] = data['X4'].str.replace(',','.').astype(float)
data['Y'] = data['Y'].str.replace(',','.').astype(float)
x = data.iloc[: , 0:4]
y = data.iloc[: , 4:]
x_train = x.iloc[200:, :]
y_train = y.iloc[200:, :]
x_test = x.iloc[:200, :]
y_test = y.iloc[:200, :]
# define the keras model
model = Sequential()
model.add(Dense(10, input_dim=4, activation='linear'))
# compile the keras model
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
# fit the keras model on the dataset
model.fit(x_train, y_train, epochs=150, batch_size=10)
# evaluate the keras model
accuracy = model.evaluate(x_train, y_train)
print(time.process_time() - start)
accuracy = model.evaluate(x_test, y_test)
